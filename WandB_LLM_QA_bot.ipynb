{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1720015a814a4008a4a9da847ab87707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9756e76e6d9b4f21b7efa5fc4d9327d0",
              "IPY_MODEL_7454ae7364a2460cad26c600e1cb8536"
            ],
            "layout": "IPY_MODEL_8cb86d5512614172b8709322b6bc6361"
          }
        },
        "9756e76e6d9b4f21b7efa5fc4d9327d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d54ebb8370e34ab5a8d84940a5b882f5",
            "placeholder": "​",
            "style": "IPY_MODEL_1193d72aa8f3433492455c4f2f58fbee",
            "value": "0.830 MB of 0.830 MB uploaded\r"
          }
        },
        "7454ae7364a2460cad26c600e1cb8536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7662a599afe74bfeb7139f79b265ba24",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f0b5aa75ff34d4eb2c4a2bf5430b860",
            "value": 1
          }
        },
        "8cb86d5512614172b8709322b6bc6361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54ebb8370e34ab5a8d84940a5b882f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1193d72aa8f3433492455c4f2f58fbee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7662a599afe74bfeb7139f79b265ba24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0b5aa75ff34d4eb2c4a2bf5430b860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building an LLM App for Document Retrieval / Extraction\n",
        "This tutorial shows how to build a basic LLM App for retrival-augmented question-answering.\n",
        "* **Dataset:** YouTube transcripts\n",
        "* **Feature store:** chromadb\n",
        "* Track datasets and embeddings as artifacts\n",
        "* Track prompts and chain executions\n",
        "* Log token counts and cost"
      ],
      "metadata": {
        "id": "0c6a_mGw42VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "LT5WBuFIFtvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ohTdWCr4yF1"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq wandb langchain pytube tiktoken openai youtube-transcript-api chromadb langchain-openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: remove this cell\n",
        "import os\n",
        "\n",
        "# set openApi key in os variables"
      ],
      "metadata": {
        "id": "kOa827wj6NzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
        "  if any([\"VSCODE\" in x for x in os.environ.keys()]):\n",
        "    print(\"Please enter password in the VS Code prompt at the top of your VS Code window!\")\n",
        "  os.environ[\"OPEN_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
        "\n",
        "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
        "print(\"OpenAI API key configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHf1HNm5ZCX",
        "outputId": "864a3683-428f-4f33-b941-6088da42bee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up config and environment variables\n",
        "* NOTE: set the entity to your username or team name\n",
        "* Set WandB environment variables to change behavior of logging\n",
        "* `ENTITY` - username or team where your projects live\n",
        "* `PROJECT` - project where your runs will live\n",
        "* `LANGCHAIN_WANDB_TRACING` - automatically logs langchain traces, inputs and outputs as part of runs in WandB."
      ],
      "metadata": {
        "id": "Vjpqy8Be6kBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "project_name = \"RAG-QA-Bot\" #@param\n",
        "entity = \"matin-kh\" #@param\n",
        "TOTAL_EPISODES = 5\n",
        "\n",
        "playlist_url = \"https://www.youtube.com/playlist?list=PLD80i8An1OEEb1jP0sjEyiLG8ULRXFob_\"\n",
        "root_data_dir = Path(\"/contents/data\")\n",
        "root_artifact_dir = Path(\"downloaded_artifacts\")\n",
        "yt_podcast_data_artifact = f\"{entity}/{project_name}/yt_podcast_transcript:latest\"\n",
        "summarized_data_artifact = f\"{entity}/{project_name}/summarized_podcasts:latest\"\n",
        "summarized_que_data_artifact = f\"{entity}/{project_name}/summarized_que_podcasts:latest\"\n",
        "transcript_embeddings_artifact = f\"{entity}/{project_name}/transcript_embeddings:latest\"\n",
        "\n",
        "os.makedirs(\"/contents/data\", exist_ok=True)\n",
        "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
        "os.environ['WANDB_PROJECT'] = project_name\n",
        "os.environ['WANDB_ENTITY'] = entity\n"
      ],
      "metadata": {
        "id": "1g2tVVCx6hWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log into WandB\n",
        "You can explicitly login using `wandb login` or `wandb.login()` (see below)\n",
        "\n",
        "Alternatively, you can set environment variables. There are several env variables which you can set to change the behavior of WandB logging. The most important ones are:\n",
        "* `WANDB_API_KEY`: available in \"Settings\" section under your WandB profile.\n",
        "* `WANDB_BASE_URL`: this is the url of the WandB server (if you are using a private instance)\n"
      ],
      "metadata": {
        "id": "Y7FOxN3L_BJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb offline\n",
        "!export CUDA_VISIBLE_DEVICES=-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS26IOdF9RYX",
        "outputId": "023e3764-180f-4ab9-9fbd-1c1da9a419bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export WANDB_INIT_TIMEOUT=120\n",
        "!export WANDB_HTTP_TIMEOUT=60"
      ],
      "metadata": {
        "id": "fPZEySEf7DzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb online"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOJXY9yOHEgz",
        "outputId": "65fc77d3-39dd-42ed-9c64-3407940c2422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "x74konSa--tR",
        "outputId": "d706dc78-4422-4a3c-e7e1-8c0981a22f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from langchain.document_loaders import YoutubeLoader\n",
        "from pytube import Playlist, YouTube\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def retry_access_yt_object(url, max_retries=5, interval_secs=5):\n",
        "    \"\"\"\n",
        "    Retries creating a YouTube object with the given URL and accessing its title several times\n",
        "    with a given interval in seconds, until it succeeds or the maximum number of attempts is reached.\n",
        "    If the object still cannot be created or the title cannot be accessed after the maximum number\n",
        "    of attempts, the last exception is raised.\n",
        "    \"\"\"\n",
        "    last_exception = None\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            yt = YouTube(url)\n",
        "            title = yt.title  # Access the title of the YouTube object.\n",
        "            return yt  # Return the YouTube object if successful.\n",
        "        except Exception as err:\n",
        "            last_exception = err  # Keep track of the last exception raised.\n",
        "            print(\n",
        "                f\"Failed to create YouTube object or access title. Retrying... ({i+1}/{max_retries})\"\n",
        "            )\n",
        "            time.sleep(interval_secs)  # Wait for the specified interval before retrying.\n",
        "\n",
        "    # If the YouTube object still cannot be created or the title cannot be accessed after the maximum number of attempts, raise the last exception.\n",
        "    raise last_exception"
      ],
      "metadata": {
        "id": "RzRxuXmV_k1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Process Data (Artifacts)"
      ],
      "metadata": {
        "id": "xDSWB0U0FxAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log Data Snapshots as Artifacts\n",
        "WandB is unopiniated with regard to how you track your experiments. We could log data in any number of ways.\n",
        "* Log one artifact which represents all the data - training, validation, and test\n",
        "* Log several artifacts - one for each of the training, validation, and test data\n",
        "\n",
        "### Anatomy of an artifact\n",
        "The `Artifact` class will correspond to an entry in the WandB Artifact registry. The artifact has\n",
        "* a name\n",
        "* a type\n",
        "* metadata\n",
        "* description\n",
        "* files, directory of files, or references\n",
        "\n",
        "Example usage\n",
        "```python\n",
        "run wandb.init(project = \"my-project\")\n",
        "artifact = wandb.Artifact(name=\"my_artifact\", type=\"data\")\n",
        "artifact.add_file(\"/PATH/TO/MY/FILE.txt\")\n",
        "run.log_artifact(artifact)\n",
        "run.finish\n",
        "```"
      ],
      "metadata": {
        "id": "oM927qGfB6x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=project_name, entity=entity, job_type=\"dataset\",\n",
        "                 settings=wandb.Settings(start_method=\"fork\"))\n",
        "\n",
        "playlist = Playlist(playlist_url)\n",
        "playlist_video_urls = playlist.video_urls[0:TOTAL_EPISODES]\n",
        "\n",
        "print(f\"There are total {len(playlist_video_urls)} videos in the playlist.\")\n",
        "\n",
        "video_data = []\n",
        "for video in tqdm(playlist_video_urls, total=len(playlist_video_urls)):\n",
        "  try:\n",
        "    curr_video_data = {}\n",
        "    yt = retry_access_yt_object(video, max_retries=25, interval_secs=2)\n",
        "    curr_video_data[\"title\"] = yt.title\n",
        "    curr_video_data[\"url\"] = video\n",
        "    curr_video_data[\"duration\"] = yt.length\n",
        "    curr_video_data[\"publish_date\"] = yt.publish_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    loader = YoutubeLoader.from_youtube_url(video)\n",
        "    transcript = loader.load()[0].page_content\n",
        "    transcript = \" \".join(transcript.split())\n",
        "    curr_video_data[\"transcript\"] = transcript\n",
        "    curr_video_data[\"total_words\"] = len(transcript.split())\n",
        "\n",
        "    video_data.append(curr_video_data)\n",
        "  except Exception as inst:\n",
        "    print(type(inst))\n",
        "    print(inst.args)\n",
        "    print(inst)\n",
        "    print(f\"Failed to scrape {video}\")\n",
        "\n",
        "print(f\"Total podcast episodes scraped: {len(video_data)}\")\n",
        "\n",
        "# save the scraped data to a csv file\n",
        "df = pd.DataFrame(video_data)\n",
        "data_path = root_data_dir / \"yt_podcast_transcript.csv\"\n",
        "df.to_csv(data_path, index=False)\n",
        "\n",
        "# upload the scraped data to wandb\n",
        "artifact = wandb.Artifact(\"yt_podcast_transcript\", type=\"dataset\")\n",
        "artifact.add_file(data_path)\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "r7AdnmUaBpOd",
        "outputId": "940ea68d-542c-465c-a6bb-26172dc03a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatin-kh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240319_234838-kholhl59</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/matin-kh/RAG-QA-Bot/runs/kholhl59' target=\"_blank\">rich-deluge-5</a></strong> to <a href='https://wandb.ai/matin-kh/RAG-QA-Bot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/matin-kh/RAG-QA-Bot' target=\"_blank\">https://wandb.ai/matin-kh/RAG-QA-Bot</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/matin-kh/RAG-QA-Bot/runs/kholhl59' target=\"_blank\">https://wandb.ai/matin-kh/RAG-QA-Bot/runs/kholhl59</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are total 5 videos in the playlist.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:07<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total podcast episodes scraped: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Artifact yt_podcast_transcript>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log (save) a WandB Table to interact with your data\n",
        "* Log the dataframe of metadata about the youtube transcripts (urls, lengths, transcripts)\n",
        "* This allows us to explore the original data (filtering, grouping, etc)"
      ],
      "metadata": {
        "id": "X5mLzqIoF2do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create WandB table\n",
        "table = wandb.Table(dataframe=df)\n",
        "run.log({\"yt_podcast_transcript\": table})\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "1720015a814a4008a4a9da847ab87707",
            "9756e76e6d9b4f21b7efa5fc4d9327d0",
            "7454ae7364a2460cad26c600e1cb8536",
            "8cb86d5512614172b8709322b6bc6361",
            "d54ebb8370e34ab5a8d84940a5b882f5",
            "1193d72aa8f3433492455c4f2f58fbee",
            "7662a599afe74bfeb7139f79b265ba24",
            "7f0b5aa75ff34d4eb2c4a2bf5430b860"
          ]
        },
        "id": "P-Au8DJXEkl9",
        "outputId": "c27a1991-f38c-4529-e64a-9b5de2f0b0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.546 MB of 0.819 MB uploaded\\r'), FloatProgress(value=0.6673353463621029, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1720015a814a4008a4a9da847ab87707"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-deluge-5</strong> at: <a href='https://wandb.ai/matin-kh/RAG-QA-Bot/runs/kholhl59' target=\"_blank\">https://wandb.ai/matin-kh/RAG-QA-Bot/runs/kholhl59</a><br/>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240319_234838-kholhl59/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarize YouTube Transcripts\n",
        "Here we summarize the transcripts in chunks, summarizing each chunk and then summarizing the summaries using the LangChain `load_summarize_chain`.\n",
        "\n",
        "We can do this in parallel since each chunk of a transcript can be summarized independently so we employ `map_reduce`."
      ],
      "metadata": {
        "id": "aNgskNwMIHjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "def get_data(artifact_name: str, total_episodes: int=None):\n",
        "  podcast_artifact = wandb.use_artifact(artifact_name)\n",
        "  podcast_artifact_dir = podcast_artifact.download(root_artifact_dir)\n",
        "  filename = artifact_name.split(\":\")[0].split(\"/\")[-1]\n",
        "  df = pd.read_csv(os.path.join(podcast_artifact_dir, f\"{filename}.csv\"))\n",
        "  if total_episodes is not None:\n",
        "    df = df.iloc[:total_episodes]\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def summarize_episodes(episode_df: pd.DataFrame):\n",
        "  # load docs into langchain format\n",
        "  loader = DataFrameLoader(episode_df, page_content_column=\"transcript\")\n",
        "  data = loader.load()\n",
        "\n",
        "  # split the documents\n",
        "  text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\n",
        "  docs = text_splitter.split_documents(data)\n",
        "  print(f\"Number of documents for podcast {data[0].metadata['title']}: {len(docs)}\")\n",
        "\n",
        "  # Initialize LLM\n",
        "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "  # define map prompt\n",
        "  map_prompt = \"\"\"Write a concise summary of the following short transcript from a podcast.\n",
        "  Don't add your opinions or interpretations.\n",
        "\n",
        "  {text}\n",
        "\n",
        "  CONCISE SUMMARY:\"\"\"\n",
        "\n",
        "  # define combine prompt\n",
        "  combine_prompt = \"\"\"You have been provided with summaries of chunks of transcripts from a podcast.\n",
        "  Your task is to merge these intermediate summaries to create a brief and comprehensive summary of the entire podcast.\n",
        "  The summary should encompass all the crucial points of the podcast.\n",
        "  Ensure that the summary is atleast 2 paragraph long and effectively captures the essence of the podcast.\n",
        "  {text}\n",
        "\n",
        "  SUMMARY:\"\"\"\n",
        "\n",
        "  map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])\n",
        "  combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])\n",
        "\n",
        "  # initialize the summarizer chain\n",
        "  chain = load_summarize_chain(\n",
        "      llm,\n",
        "      chain_type=\"map_reduce\",\n",
        "      return_intermediate_steps=True,\n",
        "      map_prompt=map_prompt_template,\n",
        "      combine_prompt=combine_prompt_template,\n",
        "  )\n",
        "\n",
        "  summary = chain({\"input_documents\": docs})\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "rd7-cpyEGK3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Summary chain and log (save) results\n",
        "* You can instantiate a `WandBTracer` and pass in additional config about this LangChain run.\n",
        "* Log the outputs of the chain like tokens used, cost, etc\n",
        "* Log the resulting summaries as artifacts."
      ],
      "metadata": {
        "id": "NHLMLpIT4Qep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.tracers import WandbTracer\n",
        "\n",
        "tracer = WandbTracer(run_args = {\"job_type\": \"summarize\"})\n",
        "\n",
        "# get scraped data\n",
        "df = get_data(artifact_name=yt_podcast_data_artifact, total_episodes=TOTAL_EPISODES)\n",
        "\n",
        "summaries = []\n",
        "with get_openai_callback() as cb:\n",
        "  for episode in tqdm(df.iterrows(), total=len(df), desc=\"Summarizing episodes\"):\n",
        "    episode_data = episode[1].to_frame().T\n",
        "\n",
        "    summary = summarize_episodes(episode_data)\n",
        "    summaries.append(summary[\"output_text\"])\n",
        "\n",
        "  print(\"*\" * 25)\n",
        "  print(cb)\n",
        "  print(\"*\" * 25)\n",
        "\n",
        "  wandb.log(\n",
        "      {\n",
        "          \"total_prompt_tokens\": cb.prompt_tokens,\n",
        "          \"total_completion_tokens\": cb.completion_tokens,\n",
        "          \"total_tokens\": cb.total_tokens,\n",
        "          \"total_cost\": cb.total_cost,\n",
        "      }\n",
        "  )\n",
        "\n",
        "df[\"summary\"] = summaries\n",
        "\n",
        "# save data\n",
        "path_to_save = os.path.join(root_data_dir, \"summarized_podcasts.csv\")\n",
        "df.to_csv(path_to_save, index=False)\n",
        "\n",
        "# log to WandB artifact\n",
        "artifact = wandb.Artifact(\"summarized_podcasts\", type=\"dataset\")\n",
        "artifact.add_file(path_to_save)\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "# create wandb table\n",
        "table = wandb.Table(dataframe=df)\n",
        "wandb.log({\"summarized_podcasts\": table})\n",
        "\n",
        "tracer.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "aAL16xDYKWo6",
        "outputId": "c18d2987-3cda-434c-f2ed-d85b81a412cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/matin-kh/RAG-QA-Bot/runs/fegegzc6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "Summarizing episodes:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents for podcast Transforming Data into Business Solutions with Salesforce AI CEO, Clara Shih: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WARNING: Failed to serialize model: 'int' object is not subscriptable\n",
            "Summarizing episodes:   0%|          | 0/5 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2f9b3d1544d1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mepisode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-325fc1e0afea>\u001b[0m in \u001b[0;36msummarize_episodes\u001b[0;34m(episode_df)\u001b[0m\n\u001b[1;32m     63\u001b[0m   )\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m   \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input_documents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    376\u001b[0m         }\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             outputs = (\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_reduce.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdone\u001b[0m \u001b[0mrecursively\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmany\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         map_results = self.llm_chain.apply(\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;31m# FYI - this is parallelized and so it is fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    570\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         flattened_outputs = [\n\u001b[1;32m    436\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 results.append(\n\u001b[0;32m--> 424\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    425\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 )\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         }\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    668\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 897\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    974\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    974\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed the contents of YouTube Transcripts\n",
        "Here we use OpenAI embeddings and ChromaDB to embed the summaries to make them queriable via vector similarity search when we ask contextual questions to the LLM.\n",
        "\n",
        "We use `wandb.log` to save the resulting ChromaDB serialized embeddings as artifacts."
      ],
      "metadata": {
        "id": "JSepuZClHyzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ChromaDB\n",
        "\n",
        "import os\n",
        "from dataclasses import asdict\n",
        "\n",
        "import pandas as pd\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from tqdm import tqdm\n",
        "from wandb.integration.langchain import WandbTracer\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "def get_data(artifact_name: str, total_episodes=None):\n",
        "    podcast_artifact = wandb.use_artifact(artifact_name, type=\"dataset\")\n",
        "    podcast_artifact_dir = podcast_artifact.download(root_artifact_dir)\n",
        "    filename = artifact_name.split(\":\")[0].split(\"/\")[-1]\n",
        "    df = pd.read_csv(os.path.join(podcast_artifact_dir, f\"{filename}.csv\"))\n",
        "    if total_episodes is not None:\n",
        "        df = df.iloc[:total_episodes]\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_embeddings(episode_df: pd.DataFrame, index: int):\n",
        "    # load docs into langchain format\n",
        "    loader = DataFrameLoader(episode_df, page_content_column=\"transcript\")\n",
        "    data = loader.load()\n",
        "\n",
        "    # split the documents\n",
        "    text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=0)\n",
        "    docs = text_splitter.split_documents(data)\n",
        "\n",
        "    title = data[0].metadata[\"title\"]\n",
        "    print(f\"Number of documents for podcast {title}: {len(docs)}\")\n",
        "\n",
        "    # initialize embedding engine\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        docs,\n",
        "        embeddings,\n",
        "        persist_directory=os.path.join(root_data_dir / \"chromadb\", str(index)),\n",
        "    )\n",
        "    db.persist()"
      ],
      "metadata": {
        "id": "vawd_MFY51b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding part\n",
        "\n",
        "tracer = WandbTracer(run_args = {\"job_type\": \"embed_transcripts\"})\n",
        "\n",
        "# get data\n",
        "df = get_data(artifact_name=summarized_data_artifact, total_episodes=TOTAL_EPISODES)\n",
        "\n",
        "# create embeddings\n",
        "with get_openai_callback() as cb:\n",
        "    for episode in tqdm(df.iterrows(), total=len(df), desc=\"Embedding transcripts\"):\n",
        "        episode_data = episode[1].to_frame().T\n",
        "\n",
        "        create_embeddings(episode_data, index=episode[0])\n",
        "\n",
        "    print(\"*\" * 25)\n",
        "    print(cb)\n",
        "    print(\"*\" * 25)\n",
        "\n",
        "    wandb.log(\n",
        "        {\n",
        "            \"total_prompt_tokens\": cb.prompt_tokens,\n",
        "            \"total_completion_tokens\": cb.completion_tokens,\n",
        "            \"total_tokens\": cb.total_tokens,\n",
        "            \"total_cost\": cb.total_cost,\n",
        "        }\n",
        "    )\n",
        "\n",
        "# log embeddings to wandb artifact\n",
        "artifact = wandb.Artifact(\"transcript_embeddings\", type=\"dataset\")\n",
        "artifact.add_dir(root_data_dir / \"chromadb\")\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "tracer.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "99NEEm4eIScZ",
        "outputId": "cb8241f0-4941-4dd1-c079-cafe7bce6c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m This feature is deprecated and has been moved to `langchain`. Enable tracing by setting LANGCHAIN_WANDB_TRACING=true in your environment. See the documentation at https://python.langchain.com/docs/ecosystem/integrations/agent_with_wandb_tracing for guidance. Replace your current import with `from langchain.callbacks.tracers import WandbTracer`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CommError",
          "evalue": "Unable to fetch artifact with name matin-kh/RAG-QA-Bot/summarized_podcasts:latest",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/public/api.py\u001b[0m in \u001b[0;36martifact\u001b[0;34m(self, name, type)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_artifact_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         artifact = wandb.Artifact._from_name(\n\u001b[0m\u001b[1;32m    959\u001b[0m             \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/artifacts/artifact.py\u001b[0m in \u001b[0;36m_from_name\u001b[0;34m(cls, entity, project, name, client)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;34mf\"Unable to fetch artifact with name {entity}/{project}/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to fetch artifact with name matin-kh/RAG-QA-Bot/summarized_podcasts:latest",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6ffab0e93626>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummarized_data_artifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOTAL_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# create embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f974ad9ef1f4>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(artifact_name, total_episodes)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpodcast_artifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpodcast_artifact_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpodcast_artifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_artifact_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martifact_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Run\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_finished\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                 default_message = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36muse_artifact\u001b[0;34m(self, artifact_or_name, type, aliases, use_as)\u001b[0m\n\u001b[1;32m   2781\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0martifact_or_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m             \u001b[0mpublic_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_public_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2783\u001b[0;31m             \u001b[0martifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpublic_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2784\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCommError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Whoa, you found a bug.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_backend_error_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/public/api.py\u001b[0m in \u001b[0;36martifact\u001b[0;34m(self, name, type)\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must specify name= to fetch an artifact.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_artifact_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         artifact = wandb.Artifact._from_name(\n\u001b[0m\u001b[1;32m    959\u001b[0m             \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/artifacts/artifact.py\u001b[0m in \u001b[0;36m_from_name\u001b[0;34m(cls, entity, project, name, client)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"artifact\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;34mf\"Unable to fetch artifact with name {entity}/{project}/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             )\n",
            "\u001b[0;31mCommError\u001b[0m: Unable to fetch artifact with name matin-kh/RAG-QA-Bot/summarized_podcasts:latest"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ask Questions Against the Summarized Documents\n",
        "Finally we tie everything together.\n",
        "1. We can pull down our ChromaDB embeddings from W&B\n",
        "2. Pass them along with a prompt template for QA to the `RetrievalQA` chain and start asking questions!"
      ],
      "metadata": {
        "id": "n7rXyvc1JGzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "def get_answer(podcast: str, question: str):\n",
        "  index = df[df[\"title\"] == podcast].index[0]\n",
        "  db_dir = os.path.join(chromadb_dir, str(index))\n",
        "  embeddings = OpenAIEmbeddings()\n",
        "  db = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
        "\n",
        "  prompt_template = \"\"\"Use the following pieces of context to answer the question.\n",
        "  If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "  Don't add your opinions or interpretations. Ensure that you complete the answer.\n",
        "  If the question is not relevant to the context, just say that it is not relevant.\n",
        "\n",
        "  CONTEXT:\n",
        "  {context}\n",
        "\n",
        "  QUESTION: {question}\n",
        "\n",
        "  ANSWER:\"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "  retriever = db.as_retriever()\n",
        "  retriever.search_kwargs[\"k\"] = 2\n",
        "\n",
        "  qa = RetrievalQA.from_chain_type(\n",
        "      llm=ChatOpenAI(temperature=0),\n",
        "      chain_type=\"stuff\",\n",
        "      retriever=retriever,\n",
        "      chain_type_kwargs={\"prompt\": prompt},\n",
        "      return_source_documents=True,\n",
        "  )\n",
        "\n",
        "  with get_openai_callback() as cb:\n",
        "      result = qa({\"query\": question})\n",
        "      print(cb)\n",
        "\n",
        "  answer = result[\"result\"]\n",
        "  return answer"
      ],
      "metadata": {
        "id": "rwWQmmcYIa7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download and read data\n",
        "api = wandb.Api()\n",
        "artifact_df = api.artifact(summarized_data_artifact)\n",
        "artifact_df.download(root_data_dir)\n",
        "\n",
        "artifact_embeddings = api.artifact(transcript_embeddings_artifact)\n",
        "chromadb_dir = artifact_embeddings.download(root_data_dir / \"chromadb\")\n",
        "\n",
        "df_path = root_data_dir / \"summarized_podcasts.csv\"\n",
        "df = pd.read_csv(df_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "C5QEdaQUJjT-",
        "outputId": "d76cc31c-5afa-4402-a3e0-82d95e8bd876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CommError",
          "evalue": "Unable to fetch artifact with name matin-kh/RAG-QA-Bot/summarized_podcasts:latest",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/public/api.py\u001b[0m in \u001b[0;36martifact\u001b[0;34m(self, name, type)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_artifact_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         artifact = wandb.Artifact._from_name(\n\u001b[0m\u001b[1;32m    959\u001b[0m             \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/artifacts/artifact.py\u001b[0m in \u001b[0;36m_from_name\u001b[0;34m(cls, entity, project, name, client)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;34mf\"Unable to fetch artifact with name {entity}/{project}/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to fetch artifact with name matin-kh/RAG-QA-Bot/summarized_podcasts:latest",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-13419e9b5a96>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download and read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0martifact_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarized_data_artifact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0martifact_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCommError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/normalize.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Whoa, you found a bug.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_backend_error_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/apis/public/api.py\u001b[0m in \u001b[0;36martifact\u001b[0;34m(self, name, type)\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must specify name= to fetch an artifact.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_artifact_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m         artifact = wandb.Artifact._from_name(\n\u001b[0m\u001b[1;32m    959\u001b[0m             \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/artifacts/artifact.py\u001b[0m in \u001b[0;36m_from_name\u001b[0;34m(cls, entity, project, name, client)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"artifact\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0;34mf\"Unable to fetch artifact with name {entity}/{project}/{name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             )\n",
            "\u001b[0;31mCommError\u001b[0m: Unable to fetch artifact with name matin-kh/RAG-QA-Bot/summarized_podcasts:latest"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"title\"].tolist()[0:TOTAL_EPISODES]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdIjKOskJleg",
        "outputId": "8bd478d5-4df7-4172-f3d0-dae68979ac39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Transforming Data into Business Solutions with Salesforce AI CEO, Clara Shih',\n",
              " 'Upgrading Your Health: Navigating AIs Future In Healthcare with John Halamka of Mayo Clinic Platform',\n",
              " 'Shaping the World of Robotics with Chelsea Finn',\n",
              " \"The Power of AI in Search with You.com's Richard Socher\",\n",
              " \"AI's Future: Investment & Impact with Sarah Guo and Elad Gil of the No Priors Podcast\"]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracer = WandbTracer(run_args = {\"job_type\": \"retriealQA\"})\n",
        "\n",
        "answer = get_answer(\"The Power of AI in Search with You.com's Richard Socher\", \"Who won the 2024 superbowl?\")\n",
        "print(answer)\n",
        "\n",
        "tracer.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "UCDSDqutJn-9",
        "outputId": "0713e596-a45b-4fbd-d624-bdac685e0e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chromadb_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f9d8e0c7da82>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWandbTracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"job_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"retriealQA\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Power of AI in Search with You.com's Richard Socher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Who won the 2024 superbowl?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-20b3898809ce>\u001b[0m in \u001b[0;36mget_answer\u001b[0;34m(podcast, question)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpodcast\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpodcast\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mdb_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchromadb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chromadb_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build an App to Serve the Model"
      ],
      "metadata": {
        "id": "QHA1MfnZJr9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from ast import literal_eval\n",
        "\n",
        "import wandb\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    playlist_url: str = \"https://www.youtube.com/playlist?list=PLD80i8An1OEEb1jP0sjEyiLG8ULRXFob_\"\n",
        "\n",
        "    # paths\n",
        "    root_data_dir: Path = Path(\"data\")\n",
        "    root_artifact_dir: Path = Path(\"downloaded_artifacts\")\n",
        "\n",
        "    # wandb\n",
        "    project_name: str = \"gradient_dissent_qabot\"\n",
        "    yt_podcast_data_artifact: str = \"gladiator/gradient_dissent_qabot/yt_podcast_transcript:latest\"\n",
        "    summarized_data_artifact: str = \"gladiator/gradient_dissent_qabot/summarized_podcasts:latest\"\n",
        "    summarized_que_data_artifact: str = (\n",
        "        \"gladiator/gradient_dissent_qabot/summarized_que_podcasts:latest\"\n",
        "    )\n",
        "    transcript_embeddings_artifact: str = (\n",
        "        \"gladiator/gradient_dissent_qabot/transcript_embeddings:latest\"\n",
        "    )\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# download and read data\n",
        "api = wandb.Api()\n",
        "artifact_df = api.artifact(config.summarized_que_data_artifact)\n",
        "artifact_df.download(config.root_data_dir)\n",
        "\n",
        "artifact_embeddings = api.artifact(config.transcript_embeddings_artifact)\n",
        "chromadb_dir = artifact_embeddings.download(config.root_data_dir / \"chromadb\")\n",
        "\n",
        "df_path = config.root_data_dir / \"summarized_que_podcasts.csv\"\n",
        "df = pd.read_csv(df_path)\n",
        "\n",
        "\n",
        "def embed_video(title: str):\n",
        "    video_url = df[df[\"title\"] == title][\"url\"].values[0]\n",
        "    match = re.search(r\"v=([-\\w]+)\", video_url)\n",
        "    video_id = match.group(1)\n",
        "    # embed video\n",
        "    video_embed = f\"<iframe width='580' height='360' src=https://www.youtube.com/embed/{video_id} frameborder='0' allowfullscreen style='width:100%; max-width:100%;'></iframe>\"\n",
        "\n",
        "    return video_embed\n",
        "\n",
        "\n",
        "def get_podcast_info(title: str):\n",
        "    # get questions\n",
        "    questions = df[df[\"title\"] == title][\"questions\"].values[0]\n",
        "    questions = literal_eval(questions)\n",
        "    que_str = \"\"\n",
        "    for que in questions:\n",
        "        que_str += f\"👉 {que}\\n\"\n",
        "\n",
        "    # get summary\n",
        "    summary = df[df[\"title\"] == title][\"summary\"].values[0]\n",
        "\n",
        "    return summary, que_str\n",
        "\n",
        "\n",
        "def get_answer(podcast: str, question: str):\n",
        "    index = df[df[\"title\"] == podcast].index[0]\n",
        "    db_dir = os.path.join(chromadb_dir, str(index))\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    db = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "    Don't add your opinions or interpretations. Ensure that you complete the answer.\n",
        "    If the question is not relevant to the context, just say that it is not relevant.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION: {question}\n",
        "\n",
        "    ANSWER:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    retriever = db.as_retriever()\n",
        "    retriever.search_kwargs[\"k\"] = 2\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=ChatOpenAI(temperature=0),\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True,\n",
        "    )\n",
        "\n",
        "    with get_openai_callback() as cb:\n",
        "        result = qa({\"query\": question})\n",
        "        print(cb)\n",
        "\n",
        "    answer = result[\"result\"]\n",
        "    return answer\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h1><center>Welcome to Gradient Dissent QA Bot 🤖</center></h1>\")\n",
        "    gr.Markdown(\n",
        "        \"#### The purpose of this QA bot is to provide answers to questions related to podcast episodes from Weights & Biases' [Gradient Dissent Podcast](https://www.youtube.com/playlist?list=PLD80i8An1OEEb1jP0sjEyiLG8ULRXFob_).\"\n",
        "    )\n",
        "    gr.Markdown(\n",
        "        \"#### First select a podcast episode and click `Get Podcast Info` to get the summary and possible questions about the podcast episode.\"\n",
        "    )\n",
        "    gr.Markdown(\n",
        "        \"#### Then ask a question about the podcast episode and click `Get Answer` to get the answer.\"\n",
        "    )\n",
        "    gr.Markdown(\n",
        "        \"#### Read the report for understanding how I built this QA bot [here](https://wandb.ai/gladiator/gradient_dissent_qabot/reports/Building-a-Q-A-Bot-for-Weights-Biases-Gradient-Dissent-Podcast--Vmlldzo0MTcyMDQz)\"\n",
        "    )\n",
        "    gr.Markdown(\n",
        "        \"#### GitHub Repo [here](https://github.com/Gladiator07/wandb-gradient-dissent-bot/tree/main)\"\n",
        "    )\n",
        "    gr.Markdown(\"<br>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=0.5):\n",
        "            dropdown = gr.Dropdown(\n",
        "                df[\"title\"].to_list(), label=\"Select a Podcast Episode\", value=df.iloc[0][\"title\"]\n",
        "            )\n",
        "            podcast_info_btn = gr.Button(\"Get Podcast Info\")\n",
        "\n",
        "            podcast_info_btn.click(\n",
        "                fn=embed_video,\n",
        "                inputs=dropdown,\n",
        "                outputs=gr.HTML(label=\"Podcast Video\"),\n",
        "            )\n",
        "\n",
        "            question_box = gr.Textbox(label=\"Ask a question about the podcast episode\")\n",
        "            with gr.Row():\n",
        "                ques_clear_btn = gr.Button(\"Clear\")\n",
        "                ques_btn = gr.Button(\"Get Answer\")\n",
        "\n",
        "            ques_btn.click(\n",
        "                fn=get_answer,\n",
        "                inputs=[dropdown, question_box],\n",
        "                outputs=gr.Textbox(label=\"Answer\"),\n",
        "            )\n",
        "            ques_clear_btn.click(lambda: None, None, question_box, queue=False)\n",
        "\n",
        "        with gr.Column(scale=0.5):\n",
        "            podcast_info_btn.click(\n",
        "                fn=get_podcast_info,\n",
        "                inputs=dropdown,\n",
        "                outputs=[\n",
        "                    gr.Text(label=\"Summary of the podcast\"),\n",
        "                    gr.Text(label=\"Some of the questions you can ask\"),\n",
        "                ],\n",
        "            )\n",
        "\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "P5g7QXOuJph4",
        "outputId": "f469edcb-6c1b-4680-d3c4-978fdf54ca99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   492 of 492 files downloaded.  \n",
            "/usr/local/lib/python3.10/dist-packages/gradio/layouts/column.py:53: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://905992333697cab128.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://905992333697cab128.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4I5skVBMJ1Lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}